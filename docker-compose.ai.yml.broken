# =============================================================================
# AI SERVICES - CHAT, AUTOMATION, AND ML TOOLS
# =============================================================================
# This file contains AI-related services that can be enabled/disabled via .env file
# Profile 'ai' = CPU services (chat UIs, search, Ollama without GPU)
# Profile 'gpu' = GPU-accelerated services (Ollama with GPU, Stable Diffusion, ComfyUI)
# Profile 'all' = All services
#
# Quick Start:
#   docker compose --profile all up -d    # Start all services (GPU required)
#   docker compose --profile ai up -d     # Start CPU-only chat services
#
# First-time setup:
#   1. Verify GPU: docker run --rm --gpus all nvidia/cuda:12.0-base nvidia-smi
#   2. Start services: docker compose --profile all up -d
#   3. Wait for model downloads (check logs: docker compose logs -f ollama-init sd-model-init)
#   4. Access Open WebUI at http://localhost:3000

services:
  # =============================================================================
  # OLLAMA - Unified LLM Backend (GPU Accelerated)
  # =============================================================================
  # All chat frontends connect to this service for LLM inference
  # Auto-downloads minimal model on first start for immediate testing
  # This version requires GPU - for CPU-only, use ollama-cpu service instead
  # =============================================================================
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    networks:
      - ai-network
      - shared-network
    profiles:
      - gpu
      - all
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    volumes:
      - type: bind
        source: ${FILES_BASE_DIR:-./files}/ai-models/ollama
        target: /root/.ollama
        bind:
          create_host_path: true
    deploy:
      resources:
        limits:
          memory: ${OLLAMA_MEMORY_LIMIT:-4g}
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_COUNT:-1}
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    labels:
      # Glance Configuration
      - glance.id=ollama
      - glance.group=AI
      - glance.name=Ollama
      - glance.icon=si:ollama
      - glance.url=/go/ollama
      - glance.description=LLM Backend
      
      # Traefik Configuration
      - traefik.enable=true
      
      # External domain (HTTPS)
      - traefik.http.routers.ollama-external.entrypoints=websecure
      - "traefik.http.routers.ollama-external.rule=Host(`ollama.${BASE_DOMAIN}`)"
      - traefik.http.routers.ollama-external.tls=true
      - traefik.http.routers.ollama-external.service=ollama
      - traefik.http.routers.ollama-external.middlewares=ai-services-auth@file
      
      # Local .lab domain (HTTP redirect)
      - traefik.http.routers.ollama-lab-http.entrypoints=web
      - "traefik.http.routers.ollama-lab-http.rule=Host(`ollama.${LAB_DOMAIN}`)"
      - traefik.http.routers.ollama-lab-http.middlewares=redirect-to-https@file
      - traefik.http.routers.ollama-lab-http.service=ollama
      
      # Local .lab domain (HTTPS)
      - traefik.http.routers.ollama-lab.entrypoints=websecure
      - "traefik.http.routers.ollama-lab.rule=Host(`ollama.${LAB_DOMAIN}`)"
      - traefik.http.routers.ollama-lab.tls=true
      - traefik.http.routers.ollama-lab.service=ollama
      
      # Service definition
      - traefik.http.services.ollama.loadbalancer.server.port=11434
      - traefik.docker.network=${TRAEFIK_DOCKER_NETWORK}

  # =============================================================================
  # OLLAMA CPU - For systems without GPU (testing/fallback)
  # =============================================================================
  ollama-cpu:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    networks:
      - ai-network
      - shared-network
    profiles:
      - ai
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    volumes:
      - type: bind
        source: ${FILES_BASE_DIR:-./files}/ai-models/ollama
        target: /root/.ollama
        bind:
          create_host_path: true
    deploy:
      resources:
        limits:
          memory: ${OLLAMA_MEMORY_LIMIT:-4g}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    labels:
      - glance.id=ollama
      - glance.group=AI
      - glance.name=Ollama (CPU)
      - glance.icon=si:ollama
      - glance.url=/go/ollama
      - glance.description=LLM Backend (CPU)

  # =============================================================================
  # OLLAMA INIT - Auto-download minimal model on first start
  # =============================================================================
  ollama-init:
    image: ollama/ollama:latest
    container_name: ollama-init
    networks:
      - ai-network
    profiles:
      - gpu
      - all
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - OLLAMA_MODEL=${OLLAMA_AUTO_PULL_MODEL:-qwen2.5:0.5b}
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        if [ -z "$${OLLAMA_MODEL}" ]; then
          echo "OLLAMA_AUTO_PULL_MODEL is empty, skipping model download"
          exit 0
        fi
        echo "Checking for model: $${OLLAMA_MODEL}"
        # Check if model already exists
        if ollama list 2>/dev/null | grep -q "$${OLLAMA_MODEL}"; then
          echo "Model $${OLLAMA_MODEL} already exists, skipping download"
          exit 0
        fi
        echo "Downloading model: $${OLLAMA_MODEL} (this may take a few minutes...)"
        ollama pull "$${OLLAMA_MODEL}"
        echo "Model download complete!"
    restart: "no"
    deploy:
      resources:
        limits:
          memory: 512m

  # =============================================================================
  # OPEN WEBUI - AI Chat Interface (connects to Ollama container)
  # =============================================================================
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: unless-stopped
    networks:
      - ai-network
      - shared-network
    profiles:
      - all
      - ai
      - gpu
    ports:
      - "${OPENWEBUI_PORT:-3000}:8080"
    volumes:
      - type: bind
        source: ${DATA_BASE_DIR:-./data}/open-webui
        target: /app/backend/data
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_HOST:-http://ollama:11434}
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY:-secret-key-change-me}
      - WEBUI_NAME=AI Chat - ${COMPUTER_NAME}
    deploy:
      resources:
        limits:
          memory: ${OPENWEBUI_MEMORY_LIMIT:-2g}
    labels:
      # Glance Configuration
      - glance.id=open-webui
      - glance.group=AI
      - glance.name=Open WebUI
      - glance.icon=si:openai
      - glance.url=/go/openwebui
      - glance.description=AI Chat Interface
      
      # Traefik Configuration
      - traefik.enable=true
      
      # External domain (HTTPS via Cloudflare)
      - traefik.http.routers.openwebui-external.entrypoints=websecure
      - "traefik.http.routers.openwebui-external.rule=Host(`openwebui.${BASE_DOMAIN}`) || Host(`open-webui.${BASE_DOMAIN}`)"
      - traefik.http.routers.openwebui-external.tls=true
      - traefik.http.routers.openwebui-external.service=openwebui
      
      # Local .lab domain (HTTP)
      - traefik.http.routers.openwebui-lab-http.entrypoints=web
      - "traefik.http.routers.openwebui-lab-http.rule=Host(`openwebui.${LAB_DOMAIN}`) || Host(`open-webui.${LAB_DOMAIN}`)"
      - traefik.http.routers.openwebui-lab-http.middlewares=redirect-to-https@file
      - traefik.http.routers.openwebui-lab-http.service=openwebui
      
      # Local .lab domain (HTTPS)
      - traefik.http.routers.openwebui-lab.entrypoints=websecure
      - "traefik.http.routers.openwebui-lab.rule=Host(`openwebui.${LAB_DOMAIN}`) || Host(`open-webui.${LAB_DOMAIN}`)"
      - traefik.http.routers.openwebui-lab.tls=true
      - traefik.http.routers.openwebui-lab.service=openwebui
      
      # Service definition
      - traefik.http.services.openwebui.loadbalancer.server.port=8080
      - traefik.docker.network=${TRAEFIK_DOCKER_NETWORK}

  # =============================================================================
  # SEARXNG - PRIVACY-FOCUSED SEARCH ENGINE
  # =============================================================================
  searxng:
    image: docker.io/searxng/searxng:latest
    container_name: searxng
    restart: unless-stopped
    networks:
      - ai-network
      - shared-network
    profiles:
      - all
      - ai
    ports:
      - "${SEARXNG_PORT:-4000}:8080"
    environment:
      - SEARXNG_BASE_URL=${SEARXNG_PROTOCOL:-https}://${SEARXNG_DOMAIN:-search-${COMPUTER_NAME}.${BASE_DOMAIN}}/
      - SEARXNG_PORT=8080
      - SEARXNG_BIND_ADDRESS=0.0.0.0
      - SEARXNG_SECRET=${SEARXNG_SECRET:-searxng-secret-key-change-me-2024}
    volumes:
      - type: bind
        source: ./config/searxng
        target: /etc/searxng
        bind:
          create_host_path: true
    deploy:
      resources:
        limits:
          memory: ${SEARXNG_MEMORY_LIMIT:-1g}
    labels:
      # Glance Configuration
      - glance.id=searxng
      - glance.group=AI
      - glance.name=SearXNG
      - glance.icon=si:searxng
      - glance.url=/go/searxng
      - glance.description=Privacy Search
      
      # Traefik Configuration
      - traefik.enable=true
      
      # External domain (HTTPS via Cloudflare - with auth)
      - traefik.http.routers.searxng-external.entrypoints=websecure
      - "traefik.http.routers.searxng-external.rule=Host(`searxng.${BASE_DOMAIN}`)"
      - traefik.http.routers.searxng-external.tls=true
      - traefik.http.routers.searxng-external.middlewares=searxng-auth@file
      - traefik.http.routers.searxng-external.service=searxng
      
      # Local .lab domain (HTTP)
      - traefik.http.routers.searxng-lab-http.entrypoints=web
      - "traefik.http.routers.searxng-lab-http.rule=Host(`searxng.${LAB_DOMAIN}`)"
      - traefik.http.routers.searxng-lab-http.middlewares=redirect-to-https@file
      - traefik.http.routers.searxng-lab-http.service=searxng
      - traefik.http.routers.searxng-lab-http.priority=21
      
      # Local .lab domain (HTTPS) - No auth for local network
      - traefik.http.routers.searxng-lab.entrypoints=websecure
      - "traefik.http.routers.searxng-lab.rule=Host(`searxng.${LAB_DOMAIN}`)"
      - traefik.http.routers.searxng-lab.tls=true
      - traefik.http.routers.searxng-lab.service=searxng
      - traefik.http.routers.searxng-lab.priority=21
      
      # Service definition
      - traefik.http.services.searxng.loadbalancer.server.port=8080
      - traefik.docker.network=${TRAEFIK_DOCKER_NETWORK}

  # =============================================================================
  # STABLE DIFFUSION - IMAGE GENERATION (GPU Required)
  # =============================================================================
  # Models stored in ./files/ai-models/stable-diffusion/
  # Auto-downloads SD 1.5 model on first start if configured
  # =============================================================================
  stable-diffusion-webui:
    image: ghcr.io/ai-dock/stable-diffusion-webui:latest
    container_name: stable-diffusion-webui
    restart: unless-stopped
    networks:
      - ai-network
      - shared-network
    profiles:
      - gpu
      - all
    ports:
      - "${STABLE_DIFFUSION_PORT:-7861}:7860"
    environment:
      - WEBUI_FLAGS=${STABLE_DIFFUSION_ARGS:---listen --xformers --enable-insecure-extension-access --api}
      - CLI_ARGS=${STABLE_DIFFUSION_ARGS:---listen --xformers --enable-insecure-extension-access --api}
    volumes:
      - type: bind
        source: ${FILES_BASE_DIR:-./files}/ai-models/stable-diffusion/models
        target: /stable-diffusion-webui/models
        bind:
          create_host_path: true
      - type: bind
        source: ${FILES_BASE_DIR:-./files}/ai-models/stable-diffusion/data
        target: /stable-diffusion-webui/data
        bind:
          create_host_path: true
      - type: bind
        source: ${FILES_BASE_DIR:-./files}/ai-models/stable-diffusion/outputs
        target: /stable-diffusion-webui/outputs
        bind:
          create_host_path: true
      - type: bind
        source: ${FILES_BASE_DIR:-./files}/ai-models/stable-diffusion/embeddings
        target: /stable-diffusion-webui/embeddings
        bind:
          create_host_path: true
    deploy:
      resources:
        limits:
          memory: ${STABLE_DIFFUSION_MEMORY_LIMIT:-6g}
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_COUNT:-1}
              capabilities: [gpu]
    labels:
      # Glance Configuration
      - glance.id=stable-diffusion
      - glance.group=AI
      - glance.name=Stable Diffusion
      - glance.icon=si:stability
      - glance.url=/go/sdwebui
      - glance.description=Image Generation
      
      - "traefik.enable=${STABLE_DIFFUSION_TRAEFIK_ENABLE:-true}"
      
      # HTTP router with redirect
      - "traefik.http.routers.sdwebui-lab-http.entrypoints=web"
      - "traefik.http.routers.sdwebui-lab-http.rule=Host(`sdwebui.${LAB_DOMAIN}`) || Host(`stablediffusion.${LAB_DOMAIN}`)"
      - "traefik.http.routers.sdwebui-lab-http.middlewares=redirect-to-https@file"
      - "traefik.http.routers.sdwebui-lab-http.service=sdwebui"
      
      # HTTPS router for .lab domain
      - "traefik.http.routers.sdwebui-lab.entrypoints=websecure"
      - "traefik.http.routers.sdwebui-lab.rule=Host(`sdwebui.${LAB_DOMAIN}`) || Host(`stablediffusion.${LAB_DOMAIN}`)"
      - "traefik.http.routers.sdwebui-lab.tls=true"
      - "traefik.http.routers.sdwebui-lab.service=sdwebui"
      
      # HTTPS router for external domain
      - "traefik.http.routers.sdwebui.entrypoints=websecure"
      - "traefik.http.routers.sdwebui.rule=Host(`sdwebui.${BASE_DOMAIN}`)"
      - "traefik.http.routers.sdwebui.tls=${TRAEFIK_TLS_ENABLE:-true}"
      - "traefik.http.routers.sdwebui.tls.certresolver=${TRAEFIK_CERT_RESOLVER:-letsencrypt}"
      - "traefik.http.routers.sdwebui.service=sdwebui"
      
      # Service definition
      - "traefik.http.services.sdwebui.loadbalancer.server.port=7860"
      - "traefik.docker.network=${TRAEFIK_DOCKER_NETWORK}"

  # =============================================================================
  # SD MODEL INIT - Auto-download minimal model on first start
  # =============================================================================
  sd-model-init:
    image: alpine:latest
    container_name: sd-model-init
    networks:
      - ai-network
    profiles:
      - gpu
      - all
    environment:
      - MODEL_URL=${STABLE_DIFFUSION_AUTO_DOWNLOAD_MODEL:-}
      - MODEL_DIR=/models/Stable-diffusion
    volumes:
      - type: bind
        source: ${FILES_BASE_DIR:-./files}/ai-models/stable-diffusion/models
        target: /models
        bind:
          create_host_path: true
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        if [ -z "$${MODEL_URL}" ]; then
          echo "STABLE_DIFFUSION_AUTO_DOWNLOAD_MODEL is empty, skipping model download"
          exit 0
        fi
        
        MODEL_NAME=$$(basename "$${MODEL_URL}")
        MODEL_PATH="$${MODEL_DIR}/$${MODEL_NAME}"
        
        # Create directory if needed
        mkdir -p "$${MODEL_DIR}"
        
        if [ -f "$${MODEL_PATH}" ]; then
          echo "Model $${MODEL_NAME} already exists, skipping download"
          exit 0
        fi
        
        echo "Installing wget..."
        apk add --no-cache wget
        
        echo "Downloading model: $${MODEL_NAME}"
        echo "From: $${MODEL_URL}"
        echo "To: $${MODEL_PATH}"
        echo "This may take 5-10 minutes for a 4GB model..."
        
        wget --progress=bar:force:noscroll -O "$${MODEL_PATH}" "$${MODEL_URL}"
        
        if [ $$? -eq 0 ]; then
          echo "Model download complete!"
          ls -la "$${MODEL_DIR}"
        else
          echo "Model download failed!"
          rm -f "$${MODEL_PATH}"
          exit 1
        fi
    restart: "no"
    deploy:
      resources:
        limits:
          memory: 256m

  # =============================================================================
  # LOCALAI - DISABLED (Use Ollama instead for unified LLM backend)
  # =============================================================================
  # LocalAI has been disabled in favor of Ollama as the single LLM backend.
  # This simplifies the stack and reduces GPU memory usage.
  # If you need LocalAI's features (GGUF format, whisper integration), 
  # uncomment and configure this service.
  # =============================================================================
  # localai:
  #   image: localai/localai:latest-cpu
  #   container_name: localai
  #   restart: unless-stopped
  #   networks:
  #     - ai-network
  #     - shared-network
  #   profiles:
  #     - ai
  #     - all
  #   ports:
  #     - "${LOCALAI_PORT:-8084}:8080"
  #   environment:
  #     - DEBUG=${LOCALAI_DEBUG:-false}
  #     - MODELS_PATH=/models
  #     - THREADS=${LOCALAI_THREADS:-4}
  #   volumes:
  #     - type: bind
  #       source: ${FILES_BASE_DIR:-./files}/ai-models/localai
  #       target: /models
  #       bind:
  #         create_host_path: true
  #   deploy:
  #     resources:
  #       limits:
  #         memory: ${LOCALAI_MEMORY_LIMIT:-4g}
  #   labels:
  #     # Glance Configuration
  #     - glance.id=localai
  #     - glance.group=AI
  #     - glance.name=LocalAI
  #     - glance.icon=si:openai
  #     - glance.url=/go/localai
  #     - glance.description=Local LLM Server
  #     
  #     # Traefik Configuration
  #     - traefik.enable=true
  #     
  #     # External domain (HTTPS via Cloudflare)
  #     - traefik.http.routers.localai-external.entrypoints=websecure
  #     - "traefik.http.routers.localai-external.rule=Host(`localai.${BASE_DOMAIN}`)"
  #     - traefik.http.routers.localai-external.tls=true
  #     - traefik.http.routers.localai-external.service=localai
  #     - traefik.http.routers.localai-external.middlewares=ai-services-auth@file
  #     
  #     # Local .lab domain (HTTP)
  #     - traefik.http.routers.localai-lab-http.entrypoints=web
  #     - "traefik.http.routers.localai-lab-http.rule=Host(`localai.${LAB_DOMAIN}`)"
  #     - traefik.http.routers.localai-lab-http.middlewares=redirect-to-https@file
  #     - traefik.http.routers.localai-lab-http.service=localai
  #     - traefik.http.routers.localai-lab-http.priority=21
  #     
  #     # Local .lab domain (HTTPS) - No auth for local network
  #     - traefik.http.routers.localai-lab.entrypoints=websecure
  #     - "traefik.http.routers.localai-lab.rule=Host(`localai.${LAB_DOMAIN}`)"
  #     - traefik.http.routers.localai-lab.tls=true
  #     - traefik.http.routers.localai-lab.service=localai
  #     - traefik.http.routers.localai-lab.priority=21
  #     
  #     # Service definition
  #     - traefik.http.services.localai.loadbalancer.server.port=8080
  #     - traefik.docker.network=${TRAEFIK_DOCKER_NETWORK}

  # GPU version of LocalAI (uncomment to use)
  # localai-gpu:
  #   image: localai/localai:latest-gpu-nvidia-cuda-12
  #   container_name: localai-gpu
  #   restart: unless-stopped
  #   networks:
  #     - ai-network
  #     - shared-network
  #   profiles:
  #     - gpu
  #   ports:
  #     - "${LOCALAI_PORT:-8081}:8080"
  #   environment:
  #     - DEBUG=${LOCALAI_DEBUG:-false}
  #     - MODELS_PATH=/models
  #   volumes:
  #     - type: bind
  #       source: ${FILES_BASE_DIR:-./files}/ai-models/localai
  #       target: /models
  #       bind:
  #         create_host_path: true
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]

  # =============================================================================
  # ANYTHINGLLM - All-in-one AI Document Chat
  # =============================================================================
  # Chat with your documents using any LLM (Ollama, OpenAI, LocalAI, etc.)
  # Built-in RAG, vector database, and document processing
  # =============================================================================
  anythingllm:
    image: mintplexlabs/anythingllm:latest
    container_name: anythingllm
    restart: unless-stopped
    networks:
      - ai-network
      - shared-network
    profiles:
      - ai
      - all
      - gpu
    ports:
      - "${ANYTHINGLLM_PORT:-3003}:3001"
    environment:
      - STORAGE_DIR=/app/server/storage
      - LLM_PROVIDER=${ANYTHINGLLM_LLM_PROVIDER:-ollama}
      - OLLAMA_BASE_PATH=${OLLAMA_HOST:-http://ollama:11434}
      - EMBEDDING_MODEL_PREF=${ANYTHINGLLM_EMBEDDING_MODEL:-nomic-embed-text}
      - VECTOR_DB=${ANYTHINGLLM_VECTOR_DB:-lancedb}
      - JWT_SECRET=${ANYTHINGLLM_JWT_SECRET:-change-this-secret-key-for-production}
      - AUTH_TOKEN=${DEFAULT_TRAEFIK_AUTH_PASS:-CHANGEME1234}
      - DISABLE_TELEMETRY=true
    volumes:
      - type: volume
        source: anythingllm-data
        target: /app/server/storage
    cap_add:
      - SYS_ADMIN
    deploy:
      resources:
        limits:
          memory: ${ANYTHINGLLM_MEMORY_LIMIT:-2g}
    labels:
      # Glance Configuration
      - glance.id=anythingllm
      - glance.group=AI
      - glance.name=AnythingLLM
      - glance.icon=si:openai
      - glance.url=/go/anythingllm
      - glance.description=AI Document Chat
      
      # Traefik Configuration
      - traefik.enable=true
      
      # External domain (HTTPS via Cloudflare)
      - traefik.http.routers.anythingllm-external.entrypoints=websecure
      - "traefik.http.routers.anythingllm-external.rule=Host(`anythingllm.${BASE_DOMAIN}`)"
      - traefik.http.routers.anythingllm-external.tls=true
      - traefik.http.routers.anythingllm-external.service=anythingllm
      
      # Local .lab domain (HTTP)
      - traefik.http.routers.anythingllm-lab-http.entrypoints=web
      - "traefik.http.routers.anythingllm-lab-http.rule=Host(`anythingllm.${LAB_DOMAIN}`)"
      - traefik.http.routers.anythingllm-lab-http.middlewares=redirect-to-https@file
      - traefik.http.routers.anythingllm-lab-http.service=anythingllm
      - traefik.http.routers.anythingllm-lab-http.priority=21
      
      # Local .lab domain (HTTPS) - No auth for local network
      - traefik.http.routers.anythingllm-lab.entrypoints=websecure
      - "traefik.http.routers.anythingllm-lab.rule=Host(`anythingllm.${LAB_DOMAIN}`)"
      - traefik.http.routers.anythingllm-lab.tls=true
      - traefik.http.routers.anythingllm-lab.service=anythingllm
      - traefik.http.routers.anythingllm-lab.priority=21
      
      # Service definition
      - traefik.http.services.anythingllm.loadbalancer.server.port=3001
      - traefik.docker.network=${TRAEFIK_DOCKER_NETWORK}

  # =============================================================================
  # WHISPER - Speech-to-Text API
  # =============================================================================
  # OpenAI Whisper for transcription and translation
  # Provides OpenAI-compatible API endpoint
  # =============================================================================
  whisper:
    image: onerahmet/openai-whisper-asr-webservice:latest
    container_name: whisper
    restart: unless-stopped
    networks:
      - ai-network
      - shared-network
    profiles:
      - ai
      - all
    ports:
      - "${WHISPER_PORT:-9002}:9000"
    environment:
      - ASR_MODEL=${WHISPER_MODEL:-base}
      - ASR_ENGINE=${WHISPER_ENGINE:-openai_whisper}
    volumes:
      - type: volume
        source: whisper-data
        target: /root/.cache
    deploy:
      resources:
        limits:
          memory: ${WHISPER_MEMORY_LIMIT:-4g}
    labels:
      # Glance Configuration
      - glance.id=whisper
      - glance.group=AI
      - glance.name=Whisper
      - glance.icon=si:openai
      - glance.url=/go/whisper
      - glance.description=Speech-to-Text
      
      # Traefik Configuration
      - traefik.enable=true
      
      # External domain (HTTPS via Cloudflare)
      - traefik.http.routers.whisper-external.entrypoints=websecure
      - "traefik.http.routers.whisper-external.rule=Host(`whisper.${BASE_DOMAIN}`)"
      - traefik.http.routers.whisper-external.tls=true
      - traefik.http.routers.whisper-external.service=whisper
      - traefik.http.routers.whisper-external.middlewares=ai-services-auth@file
      
      # Local .lab domain (HTTP)
      - traefik.http.routers.whisper-lab-http.entrypoints=web
      - "traefik.http.routers.whisper-lab-http.rule=Host(`whisper.${LAB_DOMAIN}`)"
      - traefik.http.routers.whisper-lab-http.middlewares=redirect-to-https@file
      - traefik.http.routers.whisper-lab-http.service=whisper
      - traefik.http.routers.whisper-lab-http.priority=21
      
      # Local .lab domain (HTTPS) - No auth for local network
      - traefik.http.routers.whisper-lab.entrypoints=websecure
      - "traefik.http.routers.whisper-lab.rule=Host(`whisper.${LAB_DOMAIN}`)"
      - traefik.http.routers.whisper-lab.tls=true
      - traefik.http.routers.whisper-lab.service=whisper
      - traefik.http.routers.whisper-lab.priority=21
      
      # Service definition
      - traefik.http.services.whisper.loadbalancer.server.port=9000
      - traefik.docker.network=${TRAEFIK_DOCKER_NETWORK}

  # =============================================================================
  # WHISPERX - Advanced Speech-to-Text with Speaker Diarization
  # =============================================================================
  # Enhanced Whisper with word-level timestamps and speaker identification
  # GPU recommended for best performance
  # NOTE: Image ghcr.io/jim60105/whisperx not available - commented out
  # =============================================================================
  # whisperx:
  #   image: ghcr.io/jim60105/whisperx:base
  #   container_name: whisperx
  #   restart: unless-stopped
  #   networks:
  #     - ai-network
  #     - shared-network
  #   profiles:
  #     - gpu
  #     - all
  #   ports:
  #     - "${WHISPERX_PORT:-9001}:9000"
  #   environment:
  #     - WHISPER_MODEL=${WHISPERX_MODEL:-large-v2}
  #     - HF_TOKEN=${HUGGINGFACE_TOKEN:-}
  #   volumes:
  #     - type: volume
  #       source: whisper-data
  #       target: /root/.cache
  #   deploy:
  #     resources:
  #       limits:
  #         memory: ${WHISPERX_MEMORY_LIMIT:-8g}
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  #   labels:
  #     # Glance Configuration
  #     - glance.id=whisperx
  #     - glance.group=AI
  #     - glance.name=WhisperX
  #     - glance.icon=si:openai
  #     - glance.url=/go/whisperx
  #     - glance.description=Advanced Speech-to-Text
  #     
  #     - traefik.enable=true
  #     
  #     # HTTP router with redirect
  #     - traefik.http.routers.whisperx-lab-http.entrypoints=web
  #     - traefik.http.routers.whisperx-lab-http.rule=Host(`whisperx.${LAB_DOMAIN}`)
  #     - traefik.http.routers.whisperx-lab-http.middlewares=redirect-to-https@file
  #     - traefik.http.routers.whisperx-lab-http.service=whisperx
  #     
  #     # HTTPS router for .lab domain
  #     - traefik.http.routers.whisperx-lab.entrypoints=websecure
  #     - traefik.http.routers.whisperx-lab.rule=Host(`whisperx.${LAB_DOMAIN}`)
  #     - traefik.http.routers.whisperx-lab.tls=true
  #     - traefik.http.routers.whisperx-lab.service=whisperx
  #     
  #     # HTTPS router for external domain
  #     - traefik.http.routers.whisperx.entrypoints=websecure
  #     - traefik.http.routers.whisperx.rule=Host(`whisperx.${BASE_DOMAIN}`)
  #     - traefik.http.routers.whisperx.tls=true
  #     - traefik.http.routers.whisperx.service=whisperx
  #     
  #     # Service definition
  #     - traefik.http.services.whisperx.loadbalancer.server.port=9000
  #     - traefik.docker.network=${TRAEFIK_DOCKER_NETWORK}

  # =============================================================================
  # LIBRECHAT - Multi-Provider AI Chat Interface
  # =============================================================================
  # Feature-rich chat UI supporting OpenAI, Anthropic, Google, local models
  # Includes conversation history, presets, plugins, and more
  # =============================================================================
  librechat:
    image: ghcr.io/danny-avila/librechat:latest
    container_name: librechat
    restart: unless-stopped
    networks:
      - ai-network
      - shared-network
    profiles:
      - ai
      - all
      - gpu
    ports:
      - "${LIBRECHAT_PORT:-3080}:3080"
    environment:
      - HOST=0.0.0.0
      - MONGO_URI=mongodb://librechat-db:27017/LibreChat
      - ALLOW_REGISTRATION=${LIBRECHAT_ALLOW_REGISTRATION:-true}
      - ALLOW_SOCIAL_LOGIN=false
      - ENDPOINTS=${LIBRECHAT_ENDPOINTS:-ollama}
      - OLLAMA_BASE_URL=${OLLAMA_HOST:-http://ollama:11434}
      # JWT secrets (required)
      - JWT_SECRET=${LIBRECHAT_JWT_SECRET:-librechat-jwt-secret-change-me-in-production}
      - JWT_REFRESH_SECRET=${LIBRECHAT_JWT_REFRESH_SECRET:-librechat-refresh-secret-change-me}
      - CREDS_KEY=${LIBRECHAT_CREDS_KEY:-f34be427ebb29de8d88c107a71546019}
      - CREDS_IV=${LIBRECHAT_CREDS_IV:-e2341419ec3dd3d19b13a1a87fafcbfb}
      # Add your API keys in .env
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY:-}
    volumes:
      - type: volume
        source: librechat-data
        target: /app/data
    depends_on:
      librechat-db:
        condition: service_started
    deploy:
      resources:
        limits:
          memory: ${LIBRECHAT_MEMORY_LIMIT:-1g}
    labels:
      # Glance labels
      - glance.id=librechat
      - glance.group=AI
      - glance.name=LibreChat
      - glance.icon=si:chatbot
      - glance.url=/go/librechat
      - glance.description=AI Chat Interface
      
      # Traefik labels
      - traefik.enable=true
      
      # HTTP router with redirect
      - traefik.http.routers.librechat-lab-http.entrypoints=web
      - traefik.http.routers.librechat-lab-http.rule=Host(`librechat.${LAB_DOMAIN}`)
      - traefik.http.routers.librechat-lab-http.middlewares=redirect-to-https@file
      - traefik.http.routers.librechat-lab-http.service=librechat
      
      # HTTPS router for .lab domain
      - traefik.http.routers.librechat-lab.entrypoints=websecure
      - traefik.http.routers.librechat-lab.rule=Host(`librechat.${LAB_DOMAIN}`)
      - traefik.http.routers.librechat-lab.tls=true
      - traefik.http.routers.librechat-lab.service=librechat
      
      # HTTPS router for external domain
      - traefik.http.routers.librechat.entrypoints=websecure
      - traefik.http.routers.librechat.rule=Host(`librechat.${BASE_DOMAIN}`)
      - traefik.http.routers.librechat.tls=true
      - traefik.http.routers.librechat.service=librechat
      
      # Service definition
      - traefik.http.services.librechat.loadbalancer.server.port=3080
      - traefik.docker.network=${TRAEFIK_DOCKER_NETWORK}

  librechat-db:
    image: mongo:latest
    container_name: librechat-db
    labels:
      - glance.parent=librechat
      - glance.name=MongoDB
    restart: unless-stopped
    networks:
      - ai-network
    profiles:
      - ai
      - all
      - gpu
    volumes:
      - type: volume
        source: librechat-data
        target: /data/db
    deploy:
      resources:
        limits:
          memory: 512m

  # =============================================================================
  # PRIVATEGPT - Private Document Q&A
  # =============================================================================
  # 100% private, offline-capable document ingestion and Q&A
  # No data leaves your server
  # NOTE: Image zylon-ai/private-gpt not available on Docker Hub - commented out
  # =============================================================================
  # privategpt:
  #   image: zylon-ai/private-gpt:latest
  #   container_name: privategpt
  #   restart: unless-stopped
  #   networks:
  #     - ai-network
  #     - shared-network
  #   profiles:
  #     - gpu
  #     - all
  #   ports:
  #     - "${PRIVATEGPT_PORT:-8501}:8080"
  #   environment:
  #     - PGPT_PROFILES=${PRIVATEGPT_PROFILES:-local}
  #     - PGPT_EMBED_MODE=${PRIVATEGPT_EMBED_MODE:-huggingface}
  #     - PGPT_LLM_MODE=${PRIVATEGPT_LLM_MODE:-ollama}
  #     - PGPT_OLLAMA_API_BASE=${OLLAMA_HOST:-http://ollama:11434}
  #   volumes:
  #     - type: volume
  #       source: privategpt-data
  #       target: /home/worker/app/local_data
  #   deploy:
  #     resources:
  #       limits:
  #         memory: ${PRIVATEGPT_MEMORY_LIMIT:-4g}
  #   labels:
  #     # Glance Configuration
  #     - glance.id=privategpt
  #     - glance.group=AI
  #     - glance.name=PrivateGPT
  #     - glance.icon=si:openai
  #     - glance.url=/go/privategpt
  #     - glance.description=Private Document Q&A
  #     
  #     - traefik.enable=true
  #     
  #     # HTTP router with redirect
  #     - traefik.http.routers.privategpt-lab-http.entrypoints=web
  #     - traefik.http.routers.privategpt-lab-http.rule=Host(`privategpt.${LAB_DOMAIN}`)
  #     - traefik.http.routers.privategpt-lab-http.middlewares=redirect-to-https@file
  #     - traefik.http.routers.privategpt-lab-http.service=privategpt
  #     
  #     # HTTPS router for .lab domain
  #     - traefik.http.routers.privategpt-lab.entrypoints=websecure
  #     - traefik.http.routers.privategpt-lab.rule=Host(`privategpt.${LAB_DOMAIN}`)
  #     - traefik.http.routers.privategpt-lab.tls=true
  #     - traefik.http.routers.privategpt-lab.service=privategpt
  #     
  #     # HTTPS router for external domain
  #     - traefik.http.routers.privategpt.entrypoints=websecure
  #     - traefik.http.routers.privategpt.rule=Host(`privategpt.${BASE_DOMAIN}`)
  #     - traefik.http.routers.privategpt.tls=true
  #     - traefik.http.routers.privategpt.service=privategpt
  #     
  #     # Service definition
  #     - traefik.http.services.privategpt.loadbalancer.server.port=8080
  #     - traefik.docker.network=${TRAEFIK_DOCKER_NETWORK}

  # =============================================================================
  # COMFYUI - Node-based Stable Diffusion Interface
  # =============================================================================
  # Advanced node-based workflow for image generation
  # Highly customizable with community nodes and models
  # Models stored in ./files/ai-models/comfyui/
  # =============================================================================
  comfyui:
    image: ghcr.io/ai-dock/comfyui:latest
    container_name: comfyui
    restart: unless-stopped
    networks:
      - ai-network
      - shared-network
    profiles:
      - gpu
      - all
    ports:
      - "${COMFYUI_PORT:-8188}:8188"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    volumes:
      - type: bind
        source: ${FILES_BASE_DIR:-./files}/ai-models/comfyui
        target: /opt/ComfyUI
        bind:
          create_host_path: true
    deploy:
      resources:
        limits:
          memory: ${COMFYUI_MEMORY_LIMIT:-6g}
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_COUNT:-1}
              capabilities: [gpu]
    labels:
      # Glance Configuration
      - glance.id=comfyui
      - glance.group=AI
      - glance.name=ComfyUI
      - glance.icon=si:stablediffusion
      - glance.url=/go/comfyui
      - glance.description=Node Image Gen
      
      - traefik.enable=true
      
      # HTTP router with redirect
      - traefik.http.routers.comfyui-lab-http.entrypoints=web
      - traefik.http.routers.comfyui-lab-http.rule=Host(`comfyui.${LAB_DOMAIN}`)
      - traefik.http.routers.comfyui-lab-http.middlewares=redirect-to-https@file
      - traefik.http.routers.comfyui-lab-http.service=comfyui
      
      # HTTPS router for .lab domain
      - traefik.http.routers.comfyui-lab.entrypoints=websecure
      - traefik.http.routers.comfyui-lab.rule=Host(`comfyui.${LAB_DOMAIN}`)
      - traefik.http.routers.comfyui-lab.tls=true
      - traefik.http.routers.comfyui-lab.service=comfyui
      
      # HTTPS router for external domain
      - traefik.http.routers.comfyui.entrypoints=websecure
      - traefik.http.routers.comfyui.rule=Host(`comfyui.${BASE_DOMAIN}`)
      - traefik.http.routers.comfyui.tls=true
      - traefik.http.routers.comfyui.service=comfyui
      
      # Service definition
      - traefik.http.services.comfyui.loadbalancer.server.port=8188
      - traefik.docker.network=${TRAEFIK_DOCKER_NETWORK}

  # =============================================================================
  # DIFFRHYTHM - AI Music Generation (GPU Required)
  # =============================================================================
  # Full-length song generation with latent diffusion
  # Can generate up to ~5 minute songs from lyrics and style prompts
  # Supports text-to-music, reference audio, and instrumental generation
  # Requires NVIDIA GPU with 8GB+ VRAM minimum
  # Models stored in ./files/ai-models/diffrhythm/
  # NOTE: Requires manual build - commented out for now
  # =============================================================================
  # diffrhythm:
  #   build:
  #     context: ./config/diffrhythm
  #     dockerfile: Dockerfile
  #   image: diffrhythm:local
  #   container_name: diffrhythm
  #   restart: unless-stopped
  #   networks:
  #     - ai-network
  #     - shared-network
  #   profiles:
  #     - gpu
  #     - all
  #   ports:
  #     - "${DIFFRHYTHM_PORT:-7870}:7860"
  #   environment:
  #     - GRADIO_SERVER_NAME=0.0.0.0
  #     - GRADIO_SERVER_PORT=7860
  #   volumes:
  #     - type: bind
  #       source: ${FILES_BASE_DIR:-./files}/ai-models/diffrhythm/models
  #       target: /app/models
  #       bind:
  #         create_host_path: true
  #     - type: bind
  #       source: ${FILES_BASE_DIR:-./files}/ai-models/diffrhythm/output
  #       target: /app/output
  #       bind:
  #         create_host_path: true
  #     - type: bind
  #       source: ${FILES_BASE_DIR:-./files}/ai-models/diffrhythm/input
  #       target: /app/input
  #       bind:
  #         create_host_path: true
  #   deploy:
  #     resources:
  #       limits:
  #         memory: ${DIFFRHYTHM_MEMORY_LIMIT:-8g}
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: ${GPU_COUNT:-1}
  #             capabilities: [gpu]
  #   labels:
  #     # Glance Configuration
  #     - glance.id=diffrhythm
  #     - glance.group=AI
  #     - glance.name=DiffRhythm
  #     - glance.icon=si:musicbrainz
  #     - glance.url=/go/diffrhythm
  #     - glance.description=AI Music Gen
  #     
  #     - traefik.enable=true
  #     
  #     # HTTP router with redirect
  #     - traefik.http.routers.diffrhythm-lab-http.entrypoints=web
  #     - traefik.http.routers.diffrhythm-lab-http.rule=Host(`diffrhythm.${LAB_DOMAIN}`)
  #     - traefik.http.routers.diffrhythm-lab-http.middlewares=redirect-to-https@file
  #     - traefik.http.routers.diffrhythm-lab-http.service=diffrhythm
  #     
  #     # HTTPS router for .lab domain
  #     - traefik.http.routers.diffrhythm-lab.entrypoints=websecure
  #     - traefik.http.routers.diffrhythm-lab.rule=Host(`diffrhythm.${LAB_DOMAIN}`)
  #     - traefik.http.routers.diffrhythm-lab.tls=true
  #     - traefik.http.routers.diffrhythm-lab.service=diffrhythm
  #     
  #     # HTTPS router for external domain
  #     - traefik.http.routers.diffrhythm.entrypoints=websecure
  #     - traefik.http.routers.diffrhythm.rule=Host(`diffrhythm.${BASE_DOMAIN}`)
  #     - traefik.http.routers.diffrhythm.tls=true
  #     - traefik.http.routers.diffrhythm.service=diffrhythm
  #     
  #     # Service definition
  #     - traefik.http.services.diffrhythm.loadbalancer.server.port=7860
  #     - traefik.docker.network=${TRAEFIK_DOCKER_NETWORK}

            - driver: nvidia
              count: ${GPU_COUNT:-1}
              capabilities: [gpu]
    labels:
      # Glance Configuration
      - glance.id=diffrhythm
      - glance.group=AI
      - glance.name=DiffRhythm
      - glance.icon=si:musicbrainz
      - glance.url=/go/diffrhythm
      - glance.description=AI Music Gen
      
      - traefik.enable=true
      
      # HTTP router with redirect
      - traefik.http.routers.diffrhythm-lab-http.entrypoints=web
      - traefik.http.routers.diffrhythm-lab-http.rule=Host(`diffrhythm.${LAB_DOMAIN}`)
      - traefik.http.routers.diffrhythm-lab-http.middlewares=redirect-to-https@file
      - traefik.http.routers.diffrhythm-lab-http.service=diffrhythm
      
      # HTTPS router for .lab domain
      - traefik.http.routers.diffrhythm-lab.entrypoints=websecure
      - traefik.http.routers.diffrhythm-lab.rule=Host(`diffrhythm.${LAB_DOMAIN}`)
      - traefik.http.routers.diffrhythm-lab.tls=true
      - traefik.http.routers.diffrhythm-lab.service=diffrhythm
      
      # HTTPS router for external domain
      - traefik.http.routers.diffrhythm.entrypoints=websecure
      - traefik.http.routers.diffrhythm.rule=Host(`diffrhythm.${BASE_DOMAIN}`)
      - traefik.http.routers.diffrhythm.tls=true
      - traefik.http.routers.diffrhythm.service=diffrhythm
      
      # Service definition
      - traefik.http.services.diffrhythm.loadbalancer.server.port=7860
      - traefik.docker.network=${TRAEFIK_DOCKER_NETWORK}

# =============================================================================
# VOLUMES AND NETWORKS
# =============================================================================
# NOTE: Most AI models now use bind mounts to ./files/ai-models/ for portability
# See files/README.md for instructions on mapping to network shares
# =============================================================================
volumes:
  # Application data volumes (not models)
  anythingllm-data:
  whisper-data:
  librechat-data:
  privategpt-data:
  # Legacy volumes - kept for migration, can be removed after data migration
  # stable-diffusion-models:
  # stable-diffusion-data:
  # localai-models:
  # comfyui-data:
  # comfyui-models:
  # diffrhythm-models:

networks:
  ai-network:
    name: ai-network
    driver: bridge
    external: false
