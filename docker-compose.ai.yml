# =============================================================================
# AI SERVICES - CHAT, AUTOMATION, AND ML TOOLS
# =============================================================================
# This file contains AI-related services that can be enabled/disabled via .env file
# Use AI_SERVICES variable to enable specific services: AI_SERVICES="chat,automation,search"

services:
  # =============================================================================
  # OLLAMA - Unified LLM Backend (GPU Accelerated)
  # =============================================================================
  # All chat frontends connect to this service for LLM inference
  # GPU-accelerated version for optimal performance
  # =============================================================================
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    networks:
      - ai-network
      - shared-network
    profiles:
      - gpu
      - all
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    volumes:
      - type: bind
        source: ${FILES_BASE_DIR:-./files}/ai-models/ollama
        target: /root/.ollama
        bind:
          create_host_path: true
    deploy:
      resources:
        limits:
          memory: ${OLLAMA_MEMORY_LIMIT:-4g}
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_COUNT:-1}
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    labels:
      # Glance Configuration - Hidden, shown in Monitoring widget
      - glance.id=ollama
      - glance.group=Monitoring
      - glance.name=Ollama (GPU)
      - glance.icon=si:ollama
      - glance.url=/go/ollama
      - glance.description=LLM API (GPU)
      - glance.hide=true
      
      # Traefik Configuration
      - traefik.enable=true
      
      # External domain (HTTPS)
      - traefik.http.routers.ollama-external.entrypoints=websecure
      - "traefik.http.routers.ollama-external.rule=Host(`ollama.${BASE_DOMAIN}`)"
      - traefik.http.routers.ollama-external.tls=true
      - traefik.http.routers.ollama-external.service=ollama
      - traefik.http.routers.ollama-external.middlewares=ai-services-auth@file
      
      # Local .lab domain (HTTP redirect)
      - traefik.http.routers.ollama-lab-http.entrypoints=web
      - "traefik.http.routers.ollama-lab-http.rule=Host(`ollama.${LAB_DOMAIN}`)"
      - traefik.http.routers.ollama-lab-http.middlewares=redirect-to-https@file
      - traefik.http.routers.ollama-lab-http.service=ollama
      
      # Local .lab domain (HTTPS)
      - traefik.http.routers.ollama-lab.entrypoints=websecure
      - "traefik.http.routers.ollama-lab.rule=Host(`ollama.${LAB_DOMAIN}`)"
      - traefik.http.routers.ollama-lab.tls=true
      - traefik.http.routers.ollama-lab.service=ollama
      
      # Service definition
      - traefik.http.services.ollama.loadbalancer.server.port=11434
      - traefik.docker.network=${TRAEFIK_DOCKER_NETWORK}

  # =============================================================================
  # OLLAMA CPU - CPU Fallback (No GPU Required)
  # =============================================================================
  # Use this when GPU is not available - same container name prevents conflicts
  # Start with: docker compose --profile ai up -d
  # =============================================================================
  ollama-cpu:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    networks:
      - ai-network
      - shared-network
    profiles:
      - ai
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    volumes:
      - type: bind
        source: ${FILES_BASE_DIR:-./files}/ai-models/ollama
        target: /root/.ollama
        bind:
          create_host_path: true
    deploy:
      resources:
        limits:
          memory: ${OLLAMA_MEMORY_LIMIT:-4g}
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    labels:
      # Glance Configuration - Hidden, shown in Monitoring widget
      - glance.id=ollama
      - glance.group=Monitoring
      - glance.name=Ollama (CPU)
      - glance.icon=si:ollama
      - glance.url=/go/ollama
      - glance.description=LLM API (CPU)
      - glance.hide=true
      
      # Traefik Configuration
      - traefik.enable=true
      
      # External domain (HTTPS)
      - traefik.http.routers.ollama-external.entrypoints=websecure
      - "traefik.http.routers.ollama-external.rule=Host(`ollama.${BASE_DOMAIN}`)"
      - traefik.http.routers.ollama-external.tls=true
      - traefik.http.routers.ollama-external.service=ollama
      - traefik.http.routers.ollama-external.middlewares=ai-services-auth@file
      
      # Local .lab domain (HTTP redirect)
      - traefik.http.routers.ollama-lab-http.entrypoints=web
      - "traefik.http.routers.ollama-lab-http.rule=Host(`ollama.${LAB_DOMAIN}`)"
      - traefik.http.routers.ollama-lab-http.middlewares=redirect-to-https@file
      - traefik.http.routers.ollama-lab-http.service=ollama
      
      # Local .lab domain (HTTPS)
      - traefik.http.routers.ollama-lab.entrypoints=websecure
      - "traefik.http.routers.ollama-lab.rule=Host(`ollama.${LAB_DOMAIN}`)"
      - traefik.http.routers.ollama-lab.tls=true
      - traefik.http.routers.ollama-lab.service=ollama
      
      # Service definition
      - traefik.http.services.ollama.loadbalancer.server.port=11434
      - traefik.docker.network=${TRAEFIK_DOCKER_NETWORK}

  # =============================================================================
  # OPEN WEBUI - AI Chat Interface (connects to native Ollama)
  # =============================================================================
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: unless-stopped
    networks:
      - ai-network
      - shared-network
    profiles:
      - all
      - ai
    ports:
      - "${OPENWEBUI_PORT:-3000}:8080"
    volumes:
      - type: bind
        source: ${DATA_BASE_DIR:-./data}/open-webui
        target: /app/backend/data
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_HOST:-http://host.docker.internal:11434}
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY:-secret-key-change-me}
      - WEBUI_NAME=AI Chat - ${COMPUTER_NAME}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    deploy:
      resources:
        limits:
          memory: ${OPENWEBUI_MEMORY_LIMIT:-2g}
    labels:
      # Glance Configuration
      - glance.id=open-webui
      - glance.group=AI
      - glance.name=Open WebUI
      - glance.icon=si:openai
      - glance.url=/go/openwebui
      - glance.description=AI Chat Interface
      
      # Traefik Configuration
      - traefik.enable=true
      
      # External domain (HTTPS via Cloudflare)
      - traefik.http.routers.openwebui-external.entrypoints=websecure
      - "traefik.http.routers.openwebui-external.rule=Host(`openwebui.${BASE_DOMAIN}`) || Host(`open-webui.${BASE_DOMAIN}`)"
      - traefik.http.routers.openwebui-external.tls=true
      - traefik.http.routers.openwebui-external.service=openwebui
      
      # Local .lab domain (HTTP)
      - traefik.http.routers.openwebui-lab-http.entrypoints=web
      - "traefik.http.routers.openwebui-lab-http.rule=Host(`openwebui.${LAB_DOMAIN}`) || Host(`open-webui.${LAB_DOMAIN}`)"
      - traefik.http.routers.openwebui-lab-http.middlewares=redirect-to-https@file
      - traefik.http.routers.openwebui-lab-http.service=openwebui
      
      # Local .lab domain (HTTPS)
      - traefik.http.routers.openwebui-lab.entrypoints=websecure
      - "traefik.http.routers.openwebui-lab.rule=Host(`openwebui.${LAB_DOMAIN}`) || Host(`open-webui.${LAB_DOMAIN}`)"
      - traefik.http.routers.openwebui-lab.tls=true
      - traefik.http.routers.openwebui-lab.service=openwebui
      
      # Service definition
      - traefik.http.services.openwebui.loadbalancer.server.port=8080
      - traefik.docker.network=${TRAEFIK_DOCKER_NETWORK}

  # =============================================================================
  # SEARXNG - PRIVACY-FOCUSED SEARCH ENGINE
  # =============================================================================
  searxng:
    image: docker.io/searxng/searxng:latest
    container_name: searxng
    restart: unless-stopped
    networks:
      - ai-network
      - shared-network
    profiles:
      - all
      - ai
    ports:
      - "${SEARXNG_PORT:-4000}:8080"
    environment:
      - SEARXNG_BASE_URL=${SEARXNG_PROTOCOL:-https}://${SEARXNG_DOMAIN:-search-${COMPUTER_NAME}.${BASE_DOMAIN}}/
      - SEARXNG_PORT=8080
      - SEARXNG_BIND_ADDRESS=0.0.0.0
      - SEARXNG_SECRET=${SEARXNG_SECRET:-searxng-secret-key-change-me-2024}
    volumes:
      - type: bind
        source: ./config/searxng
        target: /etc/searxng
        bind:
          create_host_path: true
    deploy:
      resources:
        limits:
          memory: ${SEARXNG_MEMORY_LIMIT:-1g}
    labels:
      # Glance Configuration
      - glance.id=searxng
      - glance.group=AI
      - glance.name=SearXNG
      - glance.icon=si:searxng
      - glance.url=/go/searxng
      - glance.description=Privacy Search
      
      # Traefik Configuration
      - traefik.enable=true
      
      # External domain (HTTPS via Cloudflare - with auth)
      - traefik.http.routers.searxng-external.entrypoints=websecure
      - "traefik.http.routers.searxng-external.rule=Host(`searxng.${BASE_DOMAIN}`)"
      - traefik.http.routers.searxng-external.tls=true
      - traefik.http.routers.searxng-external.middlewares=searxng-auth@file
      - traefik.http.routers.searxng-external.service=searxng
      
      # Local .lab domain (HTTP)
      - traefik.http.routers.searxng-lab-http.entrypoints=web
      - "traefik.http.routers.searxng-lab-http.rule=Host(`searxng.${LAB_DOMAIN}`)"
      - traefik.http.routers.searxng-lab-http.middlewares=redirect-to-https@file
      - traefik.http.routers.searxng-lab-http.service=searxng
      - traefik.http.routers.searxng-lab-http.priority=21
      
      # Local .lab domain (HTTPS) - No auth for local network
      - traefik.http.routers.searxng-lab.entrypoints=websecure
      - "traefik.http.routers.searxng-lab.rule=Host(`searxng.${LAB_DOMAIN}`)"
      - traefik.http.routers.searxng-lab.tls=true
      - traefik.http.routers.searxng-lab.service=searxng
      - traefik.http.routers.searxng-lab.priority=21
      
      # Service definition
      - traefik.http.services.searxng.loadbalancer.server.port=8080
      - traefik.docker.network=${TRAEFIK_DOCKER_NETWORK}



  # =============================================================================
  # LOCALAI - OpenAI-compatible Local LLM Server
  # =============================================================================
  # Drop-in replacement for OpenAI API, runs models locally
  # Supports llama.cpp, whisper, stable diffusion, and more
  # =============================================================================
  localai:
    image: localai/localai:latest-cpu
    container_name: localai
    restart: unless-stopped
    networks:
      - ai-network
      - shared-network
    profiles:
      - ai
      - all
    ports:
      - "${LOCALAI_PORT:-8084}:8080"
    environment:
      - DEBUG=${LOCALAI_DEBUG:-false}
      - MODELS_PATH=/models
      - THREADS=${LOCALAI_THREADS:-4}
    volumes:
      - type: volume
        source: localai-models
        target: /models
    deploy:
      resources:
        limits:
          memory: ${LOCALAI_MEMORY_LIMIT:-4g}
    labels:
      # Glance Configuration
      - glance.id=localai
      - glance.group=AI
      - glance.name=LocalAI
      - glance.icon=si:openai
      - glance.url=/go/localai
      - glance.description=Local LLM Server
      
      # Traefik Configuration
      - traefik.enable=true
      
      # External domain (HTTPS via Cloudflare)
      - traefik.http.routers.localai-external.entrypoints=websecure
      - "traefik.http.routers.localai-external.rule=Host(`localai.${BASE_DOMAIN}`)"
      - traefik.http.routers.localai-external.tls=true
      - traefik.http.routers.localai-external.service=localai
      - traefik.http.routers.localai-external.middlewares=ai-services-auth@file
      
      # Local .lab domain (HTTP)
      - traefik.http.routers.localai-lab-http.entrypoints=web
      - "traefik.http.routers.localai-lab-http.rule=Host(`localai.${LAB_DOMAIN}`)"
      - traefik.http.routers.localai-lab-http.middlewares=redirect-to-https@file
      - traefik.http.routers.localai-lab-http.service=localai
      - traefik.http.routers.localai-lab-http.priority=21
      
      # Local .lab domain (HTTPS) - No auth for local network
      - traefik.http.routers.localai-lab.entrypoints=websecure
      - "traefik.http.routers.localai-lab.rule=Host(`localai.${LAB_DOMAIN}`)"
      - traefik.http.routers.localai-lab.tls=true
      - traefik.http.routers.localai-lab.service=localai
      - traefik.http.routers.localai-lab.priority=21
      
      # Service definition
      - traefik.http.services.localai.loadbalancer.server.port=8080
      - traefik.docker.network=${TRAEFIK_DOCKER_NETWORK}

  # GPU version of LocalAI (uncomment to use)
  # localai-gpu:
  #   image: localai/localai:latest-gpu-nvidia-cuda-12
  #   container_name: localai-gpu
  #   restart: unless-stopped
  #   networks:
  #     - ai-network
  #     - shared-network
  #   profiles:
  #     - gpu
  #   ports:
  #     - "${LOCALAI_PORT:-8081}:8080"
  #   environment:
  #     - DEBUG=${LOCALAI_DEBUG:-false}
  #     - MODELS_PATH=/models
  #   volumes:
  #     - localai-models:/models
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]

  # =============================================================================
  # ANYTHINGLLM - All-in-one AI Document Chat
  # =============================================================================
  # Chat with your documents using any LLM (Ollama, OpenAI, LocalAI, etc.)
  # Built-in RAG, vector database, and document processing
  # =============================================================================
  anythingllm:
    image: mintplexlabs/anythingllm:latest
    container_name: anythingllm
    restart: unless-stopped
    networks:
      - ai-network
      - shared-network
    profiles:
      - ai
      - all
    ports:
      - "${ANYTHINGLLM_PORT:-3003}:3001"
    environment:
      - STORAGE_DIR=/app/server/storage
      - LLM_PROVIDER=${ANYTHINGLLM_LLM_PROVIDER:-ollama}
      - OLLAMA_BASE_PATH=${OLLAMA_HOST:-http://ollama:11434}
      - OLLAMA_MODEL_PREF=${ANYTHINGLLM_OLLAMA_MODEL:-qwen2.5:0.5b}
      - EMBEDDING_MODEL_PREF=${ANYTHINGLLM_EMBEDDING_MODEL:-nomic-embed-text}
      - EMBEDDING_BASE_PATH=${OLLAMA_HOST:-http://ollama:11434}
      - VECTOR_DB=${ANYTHINGLLM_VECTOR_DB:-lancedb}
      - JWT_SECRET=${ANYTHINGLLM_JWT_SECRET:-change-this-secret-key-for-production}
      - AUTH_TOKEN=${DEFAULT_TRAEFIK_AUTH_PASS:-CHANGEME1234}
      - DISABLE_TELEMETRY=true
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - type: volume
        source: anythingllm-data
        target: /app/server/storage
    cap_add:
      - SYS_ADMIN
    deploy:
      resources:
        limits:
          memory: ${ANYTHINGLLM_MEMORY_LIMIT:-2g}
    labels:
      # Glance Configuration
      - glance.id=anythingllm
      - glance.group=AI
      - glance.name=AnythingLLM
      - glance.icon=si:openai
      - glance.url=/go/anythingllm
      - glance.description=AI Document Chat
      
      # Traefik Configuration
      - traefik.enable=true
      
      # External domain (HTTPS via Cloudflare)
      - traefik.http.routers.anythingllm-external.entrypoints=websecure
      - "traefik.http.routers.anythingllm-external.rule=Host(`anythingllm.${BASE_DOMAIN}`)"
      - traefik.http.routers.anythingllm-external.tls=true
      - traefik.http.routers.anythingllm-external.service=anythingllm
      
      # Local .lab domain (HTTP)
      - traefik.http.routers.anythingllm-lab-http.entrypoints=web
      - "traefik.http.routers.anythingllm-lab-http.rule=Host(`anythingllm.${LAB_DOMAIN}`)"
      - traefik.http.routers.anythingllm-lab-http.middlewares=redirect-to-https@file
      - traefik.http.routers.anythingllm-lab-http.service=anythingllm
      - traefik.http.routers.anythingllm-lab-http.priority=21
      
      # Local .lab domain (HTTPS) - No auth for local network
      - traefik.http.routers.anythingllm-lab.entrypoints=websecure
      - "traefik.http.routers.anythingllm-lab.rule=Host(`anythingllm.${LAB_DOMAIN}`)"
      - traefik.http.routers.anythingllm-lab.tls=true
      - traefik.http.routers.anythingllm-lab.service=anythingllm
      - traefik.http.routers.anythingllm-lab.priority=21
      
      # Service definition
      - traefik.http.services.anythingllm.loadbalancer.server.port=3001
      - traefik.docker.network=${TRAEFIK_DOCKER_NETWORK}

  # =============================================================================
  # WHISPER - Speech-to-Text API
  # =============================================================================
  # OpenAI Whisper for transcription and translation
  # Provides OpenAI-compatible API endpoint
  # =============================================================================
  whisper:
    image: onerahmet/openai-whisper-asr-webservice:latest
    container_name: whisper
    restart: unless-stopped
    networks:
      - ai-network
      - shared-network
    profiles:
      - ai
      - all
    ports:
      - "${WHISPER_PORT:-9002}:9000"
    environment:
      - ASR_MODEL=${WHISPER_MODEL:-base}
      - ASR_ENGINE=${WHISPER_ENGINE:-openai_whisper}
    volumes:
      - type: volume
        source: whisper-data
        target: /root/.cache
    deploy:
      resources:
        limits:
          memory: ${WHISPER_MEMORY_LIMIT:-4g}
    labels:
      # Glance Configuration - Hidden, shown in Monitoring widget
      - glance.id=whisper
      - glance.group=Monitoring
      - glance.name=Whisper
      - glance.icon=si:openai
      - glance.url=/go/whisper
      - glance.description=Speech-to-Text API
      - glance.hide=true
      
      # Traefik Configuration
      - traefik.enable=true
      
      # External domain (HTTPS via Cloudflare)
      - traefik.http.routers.whisper-external.entrypoints=websecure
      - "traefik.http.routers.whisper-external.rule=Host(`whisper.${BASE_DOMAIN}`)"
      - traefik.http.routers.whisper-external.tls=true
      - traefik.http.routers.whisper-external.service=whisper
      - traefik.http.routers.whisper-external.middlewares=ai-services-auth@file
      
      # Local .lab domain (HTTP)
      - traefik.http.routers.whisper-lab-http.entrypoints=web
      - "traefik.http.routers.whisper-lab-http.rule=Host(`whisper.${LAB_DOMAIN}`)"
      - traefik.http.routers.whisper-lab-http.middlewares=redirect-to-https@file
      - traefik.http.routers.whisper-lab-http.service=whisper
      - traefik.http.routers.whisper-lab-http.priority=21
      
      # Local .lab domain (HTTPS) - No auth for local network
      - traefik.http.routers.whisper-lab.entrypoints=websecure
      - "traefik.http.routers.whisper-lab.rule=Host(`whisper.${LAB_DOMAIN}`)"
      - traefik.http.routers.whisper-lab.tls=true
      - traefik.http.routers.whisper-lab.service=whisper
      - traefik.http.routers.whisper-lab.priority=21
      
      # Service definition
      - traefik.http.services.whisper.loadbalancer.server.port=9000
      - traefik.docker.network=${TRAEFIK_DOCKER_NETWORK}

  # =============================================================================
  # WHISPERX - Advanced Speech-to-Text with Speaker Diarization
  # =============================================================================
  # Enhanced Whisper with word-level timestamps and speaker identification
  # GPU recommended for best performance
  # =============================================================================
  whisperx:
    image: ghcr.io/jim60105/whisperx:base
    container_name: whisperx
    restart: unless-stopped
    networks:
      - ai-network
      - shared-network
    profiles:
      - gpu
    ports:
      - "${WHISPERX_PORT:-9001}:9000"
    environment:
      - WHISPER_MODEL=${WHISPERX_MODEL:-large-v2}
      - HF_TOKEN=${HUGGINGFACE_TOKEN:-}
    volumes:
      - type: volume
        source: whisper-data
        target: /root/.cache
    deploy:
      resources:
        limits:
          memory: ${WHISPERX_MEMORY_LIMIT:-8g}
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    labels:
      - traefik.enable=true
      
      # HTTP router with redirect
      - traefik.http.routers.whisperx-lab-http.entrypoints=web
      - traefik.http.routers.whisperx-lab-http.rule=Host(`whisperx.${LAB_DOMAIN}`)
      - traefik.http.routers.whisperx-lab-http.middlewares=redirect-to-https@file
      - traefik.http.routers.whisperx-lab-http.service=whisperx
      
      # HTTPS router for .lab domain
      - traefik.http.routers.whisperx-lab.entrypoints=websecure
      - traefik.http.routers.whisperx-lab.rule=Host(`whisperx.${LAB_DOMAIN}`)
      - traefik.http.routers.whisperx-lab.tls=true
      - traefik.http.routers.whisperx-lab.service=whisperx
      
      # HTTPS router for external domain
      - traefik.http.routers.whisperx.entrypoints=websecure
      - traefik.http.routers.whisperx.rule=Host(`whisperx.${BASE_DOMAIN}`)
      - traefik.http.routers.whisperx.tls=true
      - traefik.http.routers.whisperx.service=whisperx
      
      # Service definition
      - traefik.http.services.whisperx.loadbalancer.server.port=9000
      - traefik.docker.network=${TRAEFIK_DOCKER_NETWORK}

  # =============================================================================
  # LIBRECHAT - Multi-Provider AI Chat Interface
  # =============================================================================
  # Feature-rich chat UI supporting OpenAI, Anthropic, Google, local models
  # Includes conversation history, presets, plugins, and more
  # =============================================================================
  librechat:
    image: ghcr.io/danny-avila/librechat:latest
    container_name: librechat
    restart: unless-stopped
    networks:
      - ai-network
      - shared-network
    profiles:
      - ai
      - all
    ports:
      - "${LIBRECHAT_PORT:-3080}:3080"
    environment:
      - HOST=0.0.0.0
      - MONGO_URI=mongodb://librechat-db:27017/LibreChat
      - ALLOW_REGISTRATION=${LIBRECHAT_ALLOW_REGISTRATION:-true}
      - ALLOW_SOCIAL_LOGIN=false
      - ENDPOINTS=${LIBRECHAT_ENDPOINTS:-openAI,ollama}
      - OLLAMA_BASE_URL=${OLLAMA_HOST:-http://host.docker.internal:11434}
      # JWT secrets (required)
      - JWT_SECRET=${LIBRECHAT_JWT_SECRET:-librechat-jwt-secret-change-me-in-production}
      - JWT_REFRESH_SECRET=${LIBRECHAT_JWT_REFRESH_SECRET:-librechat-refresh-secret-change-me}
      - CREDS_KEY=${LIBRECHAT_CREDS_KEY:-f34be427ebb29de8d88c107a71546019}
      - CREDS_IV=${LIBRECHAT_CREDS_IV:-e2341419ec3dd3d19b13a1a87fafcbfb}
      # Add your API keys in .env
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY:-}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - type: volume
        source: librechat-data
        target: /app/data
    depends_on:
      - librechat-db
    deploy:
      resources:
        limits:
          memory: ${LIBRECHAT_MEMORY_LIMIT:-1g}
    labels:
      # Glance labels
      - glance.id=librechat
      - glance.group=AI
      - glance.name=LibreChat
      - glance.icon=si:chatbot
      - glance.url=/go/librechat
      - glance.description=AI Chat Interface
      
      # Traefik labels
      - traefik.enable=true
      
      # HTTP router with redirect
      - traefik.http.routers.librechat-lab-http.entrypoints=web
      - traefik.http.routers.librechat-lab-http.rule=Host(`librechat.${LAB_DOMAIN}`)
      - traefik.http.routers.librechat-lab-http.middlewares=redirect-to-https@file
      - traefik.http.routers.librechat-lab-http.service=librechat
      
      # HTTPS router for .lab domain
      - traefik.http.routers.librechat-lab.entrypoints=websecure
      - traefik.http.routers.librechat-lab.rule=Host(`librechat.${LAB_DOMAIN}`)
      - traefik.http.routers.librechat-lab.tls=true
      - traefik.http.routers.librechat-lab.service=librechat
      
      # HTTPS router for external domain
      - traefik.http.routers.librechat.entrypoints=websecure
      - traefik.http.routers.librechat.rule=Host(`librechat.${BASE_DOMAIN}`)
      - traefik.http.routers.librechat.tls=true
      - traefik.http.routers.librechat.service=librechat
      
      # Service definition
      - traefik.http.services.librechat.loadbalancer.server.port=3080
      - traefik.docker.network=${TRAEFIK_DOCKER_NETWORK}

  librechat-db:
    image: mongo:latest
    container_name: librechat-db
    labels:
      - glance.parent=librechat
      - glance.name=MongoDB
    restart: unless-stopped
    networks:
      - ai-network
    profiles:
      - ai
      - all
    volumes:
      - type: volume
        source: librechat-data
        target: /data/db
    deploy:
      resources:
        limits:
          memory: 512m

  # =============================================================================
  # PRIVATEGPT - Private Document Q&A
  # =============================================================================
  # 100% private, offline-capable document ingestion and Q&A
  # No data leaves your server
  # =============================================================================
  privategpt:
    image: zylon-ai/private-gpt:latest
    container_name: privategpt
    restart: unless-stopped
    networks:
      - ai-network
      - shared-network
    profiles:
      - gpu
    ports:
      - "${PRIVATEGPT_PORT:-8501}:8080"
    environment:
      - PGPT_PROFILES=${PRIVATEGPT_PROFILES:-local}
      - PGPT_EMBED_MODE=${PRIVATEGPT_EMBED_MODE:-huggingface}
      - PGPT_LLM_MODE=${PRIVATEGPT_LLM_MODE:-ollama}
      - PGPT_OLLAMA_API_BASE=${OLLAMA_HOST:-http://host.docker.internal:11434}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - type: volume
        source: privategpt-data
        target: /home/worker/app/local_data
    deploy:
      resources:
        limits:
          memory: ${PRIVATEGPT_MEMORY_LIMIT:-4g}
    labels:
      - traefik.enable=true
      
      # HTTP router with redirect
      - traefik.http.routers.privategpt-lab-http.entrypoints=web
      - traefik.http.routers.privategpt-lab-http.rule=Host(`privategpt.${LAB_DOMAIN}`)
      - traefik.http.routers.privategpt-lab-http.middlewares=redirect-to-https@file
      - traefik.http.routers.privategpt-lab-http.service=privategpt
      
      # HTTPS router for .lab domain
      - traefik.http.routers.privategpt-lab.entrypoints=websecure
      - traefik.http.routers.privategpt-lab.rule=Host(`privategpt.${LAB_DOMAIN}`)
      - traefik.http.routers.privategpt-lab.tls=true
      - traefik.http.routers.privategpt-lab.service=privategpt
      
      # HTTPS router for external domain
      - traefik.http.routers.privategpt.entrypoints=websecure
      - traefik.http.routers.privategpt.rule=Host(`privategpt.${BASE_DOMAIN}`)
      - traefik.http.routers.privategpt.tls=true
      - traefik.http.routers.privategpt.service=privategpt
      
      # Service definition
      - traefik.http.services.privategpt.loadbalancer.server.port=8080
      - traefik.docker.network=${TRAEFIK_DOCKER_NETWORK}



# =============================================================================
# VOLUMES AND NETWORKS
# =============================================================================
volumes:
  localai-models:
  anythingllm-data:
  whisper-data:
  librechat-data:
  privategpt-data:

networks:
  ai-network:
    name: ai-network
    driver: bridge
    external: false
